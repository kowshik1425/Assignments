{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "Bayes' theorem, named after mathematician Thomas Bayes, is a fundamental concept in probability theory and statistics. It deals with conditional probabilities, which means it helps you calculate the likelihood of an event (A) happening given that you already know another event (B) has occurred.\n",
    "\n",
    "Here's a breakdown:\n",
    "\n",
    "P(A): Probability of event A happening (prior probability)\n",
    "P(B): Probability of event B happening (prior probability)\n",
    "P(A|B): Probability of event A happening given that event B has already happened (posterior probability)\n",
    "The theorem provides a formula to calculate this posterior probability:\n",
    "\n",
    "P(A|B) = ( P(B|A) * P(A) ) / P(B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab150d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "P(A|B) = ( P(B|A) * P(A) ) / P(B)\n",
    "where:\n",
    "P(A|B): This is the probability of event A happening given that event B has already happened. It's called the posterior probability.\n",
    "P(B|A): This is the probability of event B happening given that event A has already happened. It's sometimes called the likelihood.\n",
    "P(A): This is the initial probability of event A happening, independent of B (prior probability).\n",
    "P(B): This is the probability of event B happening independent of A (prior probability)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52efac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "Bayes' theorem finds applications in various fields by allowing us to update probabilities based on new evidence. Here are some common examples:\n",
    "\n",
    "1. Spam Filtering:\n",
    "Event A: Email is spam.\n",
    "Event B: Email contains certain keywords associated with spam.\n",
    "Imagine an email filtering system. Bayes' theorem can be used to calculate the probability of an email being spam (A) considering the presence of suspicious keywords (B). The filter would have:\n",
    "\n",
    "Prior probability (P(A)): This is the overall spam rate in emails received.\n",
    "Likelihood (P(B|A)): This is the probability of finding spam keywords in a known spam email.\n",
    "Probability of keywords (P(B)): This is the probability of finding these keywords in any email (including non-spam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "Conditional Probability:Conditional probability deals with the likelihood of one event (A) happening given that another event (B) has already occurred. It's denoted by P(A|B) and represents how the probability of A is affected by knowing B is true.\n",
    "\n",
    "Bayes' Theorem:Bayes' theorem provides a formula to calculate this conditional probability P(A|B) based on the following:\n",
    "P(B|A): The likelihood (probability of B happening given A is true).\n",
    "P(A): The prior probability (initial probability of A happening independent of B).\n",
    "P(B): The prior probability (initial probability of B happening independent of A).\n",
    "\n",
    "Relationship:\n",
    "=>Bayes' theorem is a tool to compute specific conditional probabilities. It allows you to calculate the probability of event A happening after you've learned that event B already happened.\n",
    "=>Conditional probability is a broader concept. It encompasses any situation where you're interested in the probability of one event considering the occurrence of another event. Bayes' theorem provides a way to calculate specific conditional probabilities when you have some additional information about the events involved (likelihood and prior probabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5c0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "Data Characteristics:\n",
    "Feature Types:\n",
    "Bernoulli Naive Bayes: Suitable for binary features (0/1 or yes/no). Examples include word presence/absence in text classification or customer churn (active/inactive).\n",
    "Multinomial Naive Bayes: Ideal for count data where features represent the frequency of events. This is commonly used for document classification where features represent word counts in documents.\n",
    "Gaussian Naive Bayes: Well-suited for continuous numerical features that are assumed to follow a normal distribution. Applications include spam filtering based on email length or financial forecasting.\n",
    "Class Distribution: Consider if the classes in your data are balanced or imbalanced. Some Naive Bayes variations like Complement Naive Bayes might be better suited for imbalanced datasets.\n",
    "\n",
    "Problem Domain:\n",
    "Text Classification: Multinomial Naive Bayes is a popular choice for text classification tasks due to its effectiveness in handling word frequency data.\n",
    "Spam Filtering: Both Multinomial and Bernoulli Naive Bayes can be used for spam filtering, depending on how you represent features (word presence/absence vs word counts).\n",
    "Numerical Data Analysis: Gaussian Naive Bayes is a good option for problems involving continuous numerical features, like financial modeling or sensor data analysis.\n",
    "\n",
    "Model Complexity vs. Performance:Naive Bayes classifiers are generally simpler to implement and train compared to more complex models. However, this simplicity comes with a trade-off in potential performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd1392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "1. Calculate Class Probabilities:Since we're assuming equal prior probabilities for classes A and B, both will have a starting probability of 0.5 (assuming there are only two classes).\n",
    "\n",
    "2. Calculate Feature Probabilities:We need to calculate the probabilities of the new instance's features (X1 = 3 and X2 = 4) given each class (A and B).\n",
    "\n",
    "For Class A:\n",
    "P(X1 = 3 | Class A) = Frequency of X1 = 3 in Class A / Total instances in Class A = 4 / 17\n",
    "P(X2 = 4 | Class A) = Frequency of X2 = 4 in Class A / Total instances in Class A = 3 / 17\n",
    "\n",
    "For Class B:\n",
    "P(X1 = 3 | Class B) = Frequency of X1 = 3 in Class B / Total instances in Class B = 1 / 7\n",
    "P(X2 = 4 | Class B) = Frequency of X2 = 4 in Class B / Total instances in Class B = 3 / 7\n",
    "\n",
    "3. Apply Naive Bayes Assumption:Naive Bayes assumes features are independent. Therefore, to calculate the probability of the new instance belonging to each class, we multiply the individual feature probabilities for that class.\n",
    "\n",
    "Class A Probability:\n",
    "P(New Instance | Class A) = P(X1 = 3 | Class A) * P(X2 = 4 | Class A) = (4/17) * (3/17) ≈ 0.068\n",
    "Class B Probability:\n",
    "P(New Instance | Class B) = P(X1 = 3 | Class B) * P(X2 = 4 | Class B) = (1/7) * (3/7) ≈ 0.143\n",
    "\n",
    "4. Determine the Most Probable Class:By comparing the probabilities, we see that the new instance has a higher probability of belonging to Class B (0.143) compared to Class A (0.068).\n",
    "\n",
    "Therefore, Naive Bayes would predict the new instance (X1 = 3, X2 = 4) to belong to Class B."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
