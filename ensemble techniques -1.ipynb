{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b6544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\n",
    "Ensemble techniques in machine learning are a powerful approach that involves combining multiple models to improve overall performance on a specific task. It's like consulting multiple experts for a decision, aiming to get a more robust and accurate outcome compared to relying on a single model.\n",
    "\n",
    "how ensemble methods work:\n",
    "\n",
    "Training Multiple Models: The core idea is to train a collection of individual models (called base learners) on the same dataset. These base learners can be of the same type (e.g., multiple decision trees) or different types of models (e.g., decision trees, neural networks).\n",
    "\n",
    "Diversity is Key:  For ensemble methods to be effective, the base learners should be diverse in their approaches to learning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715c7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2\n",
    "Reduced Variance:  Individual machine learning models can be prone to high variance, meaning small changes in the training data can lead to significant changes in the model's predictions. This can result in overfitting, where the model performs well on the training data but poorly on unseen data. Ensemble methods help reduce variance by averaging the predictions of multiple models, each potentially capturing slightly different aspects of the data. This leads to more stable and generalizable models that perform well on unseen data.\n",
    "\n",
    "Improved Accuracy: By combining the strengths of different models, ensembles can often achieve better accuracy than individual models. This is because individual models might have biases or weaknesses that other models can compensate for. For instance, a decision tree ensemble (like a random forest) can leverage the strengths of multiple decision trees, each potentially capturing different decision boundaries in the data.\n",
    "\n",
    "Addressing Model Complexity:  Ensemble methods offer a way to leverage the benefits of complex models (like decision trees or deep neural networks) while mitigating their risks. By combining multiple simpler models (like base learners in a random forest), ensembles can achieve good performance without necessarily being overly complex and prone to overfitting.\n",
    "\n",
    "Robustness to Outliers:  Outliers in the data can significantly skew the training process of a single model. Ensemble methods, by combining predictions from multiple models, can be less susceptible to the influence of outliers. Even if some models are misled by outliers, others might still capture the underlying patterns in the data, leading to more robust predictions overall.\n",
    "\n",
    "Leveraging Different Learner Types: Ensembles allow you to harness the strengths of different machine learning algorithms within a single framework. By combining models with different learning paradigms (e.g., decision trees with support vector machines), you can potentially capture a wider range of patterns in the data and improve overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d62521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "Bagging, short for bootstrap aggregating, is a popular ensemble technique in machine learning used to improve the stability and accuracy of models, particularly decision trees. Here's a breakdown of how it works:\n",
    "\n",
    "Core Idea:Train multiple versions of the same model (called base learners) on different subsets of the data with replacement. This means data points can be chosen more than once for a particular base learner.\n",
    "\n",
    "Diversity through Data Subsampling:By using random subsets of the data with replacement, each base learner encounters a slightly different training set. This injects diversity into the ensemble, making the final predictions less prone to the biases of any single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a6a552",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "Boosting is another powerful ensemble technique in machine learning that aims to improve model performance by sequentially training multiple models. Unlike bagging, which focuses on diversity through data manipulation, boosting trains models iteratively, where each subsequent model focuses on correcting the errors of the previous one.\n",
    "Core Idea:\n",
    "Start with a weak learner: Train a simple model (weak learner) on the original data. This initial model can have high bias (underfitting) but doesn't necessarily need to be very accurate.\n",
    "Analyze errors: Analyze the predictions made by the first model and identify the data points it misclassified.\n",
    "Boosting the learner: Train a second model on the same data, but with a higher weight placed on the data points that the first model misclassified. This forces the second model to focus on learning these challenging instances.\n",
    "Combine predictions: Combine the predictions from both models using a weighting scheme. The new model with better performance on the weighted data points gets a higher weight in the final ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7e4cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5\n",
    "1. Reduced Variance:\n",
    "Individual models, particularly decision trees, can be prone to high variance. Small changes in the training data can significantly impact their predictions, leading to overfitting.\n",
    "Ensemble methods like bagging average the predictions from multiple models, each potentially capturing slightly different aspects of the data. This reduces the variance and leads to more stable and generalizable models that perform well on unseen data.\n",
    "\n",
    "2. Improved Accuracy:\n",
    "By combining the strengths of different models, ensembles can often achieve better accuracy than individual models. This is because individual models might have biases or weaknesses that other models can compensate for.\n",
    "For instance, a random forest (a bagging ensemble) combines multiple decision trees, each potentially capturing different decision boundaries in the data.\n",
    "Boosting algorithms like AdaBoost train models sequentially, where each subsequent model focuses on correcting the errors of the previous one, leading to a more accurate ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed614140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6\n",
    "Ensemble techniques are powerful tools in machine learning, but they're not a guaranteed silver bullet. Here's a breakdown of when they shine and when individual models might be sufficient:\n",
    "\n",
    "Advantages of Ensembles:\n",
    "=>Improved Performance: Ensembles often outperform individual models in terms of accuracy, stability, and robustness, especially for complex problems.\n",
    "=>Reduced Variance: By averaging predictions, ensembles mitigate the high variance of models like decision trees, leading to more generalizable results.\n",
    "=>Can Leverage Different Models: Ensembles allow you to combine the strengths of various machine learning algorithms, potentially capturing a wider range of patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5408cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7\n",
    "1. Resample the Data:\n",
    "Take your original dataset and draw samples with replacement. This means you can pick the same data point multiple times in a single bootstrap sample.\n",
    "The number of data points in each bootstrap sample typically matches the size of your original data. You'll repeat this resampling process many times (e.g., 1000 times) to create a collection of bootstrap samples.\n",
    "\n",
    "2. Calculate the Statistic of Interest:For each bootstrap sample, calculate the statistic you're interested in, such as the mean, median, proportion, or a more complex function. This gives you a collection of statistic values, one for each bootstrap sample.\n",
    "\n",
    "3. Analyze the Bootstrap Distribution:The collection of statistic values from your bootstrap samples represents an estimate of the sampling distribution of the statistic. This distribution shows the possible values the statistic could take if you repeatedly sampled from the population with replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3790e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8\n",
    "Bootstrap is a statistical technique used to estimate various properties of populations (like the mean, median, or proportions) and their variability. It works by resampling the data you have at hand, creating a simulated sense of the broader population. Here's a breakdown of how it works and the steps involved:\n",
    "\n",
    "Core Idea:\n",
    "Imagine you have a limited dataset, like a sample of people from a town. You can't directly measure the characteristics of the entire town (population) from this sample.\n",
    "Bootstrapping allows you to create a simulated version of the population by resampling your data with replacement. This means you can pick the same data point multiple times in a single resampled set.\n",
    "\n",
    "Steps Involved:\n",
    "Resample the Data:Take your original dataset and draw samples with replacement. The number of data points in each resampled set (called a bootstrap sample) typically matches the size of your original data.\n",
    "Repeat this resampling process many times (e.g., 1000 times) to create a collection of bootstrap samples.\n",
    "\n",
    "Calculate the Statistic of Interest:For each bootstrap sample, calculate the statistic you're interested in. This could be the mean, median, proportion, or a more complex function relevant to your analysis.\n",
    "\n",
    "Analyze the Bootstrap Distribution:The collection of statistic values from your bootstrap samples represents an estimate of the sampling distribution of the statistic. This distribution shows the possible values the statistic could take if you repeatedly sampled from the population with replacement.\n",
    "\n",
    "Use the Bootstrap Distribution:\n",
    "This distribution allows you to estimate various properties of the population statistic, such as:\n",
    "Confidence Intervals: You can calculate a range where you expect the true population value to fall with a certain level of confidence (e.g., 95% confidence interval).\n",
    "Standard Error: This reflects the variability of the statistic across different samples.\n",
    "Hypothesis Testing: By comparing the statistic from your original data to the bootstrap distribution, you can perform hypothesis tests to assess the likelihood of observing a particular value if a certain null hypothesis were true.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "678835d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sample mean: 15.234693877551015\n",
      "95% confidence interval for population mean height: 15.036734693877555 - 15.43673469387755\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "import random\n",
    "tree_heights = [14.5, 16.2, 14.8, 13.9, 15.1, 17.0, 15.5, 14.3, 15.8, 14.0,\n",
    "                15.3, 16.7, 15.0, 14.1, 14.9, 16.4, 15.9, 14.7, 15.2, 14.4,\n",
    "                14.6, 16.1, 15.6, 14.2, 15.7, 16.8, 15.4, 14.8, 14.0, 15.0,\n",
    "                16.5, 15.0, 14.2, 14.5, 15.9, 16.3, 15.8, 14.9, 15.4, 14.1,\n",
    "                14.7, 16.0, 15.5, 14.3, 15.6, 16.9, 15.3, 14.6, 15.1]\n",
    "num_samples = 1000\n",
    "def draw_bootstrap_sample(data):\n",
    "  return random.choices(data, k=len(data))\n",
    "bootstrap_means = []\n",
    "for _ in range(num_samples):\n",
    "  sample = draw_bootstrap_sample(tree_heights)\n",
    "  bootstrap_means.append(sum(sample) / len(sample))\n",
    "bootstrap_means.sort()\n",
    "confidence_level = 0.95\n",
    "alpha = 1 - confidence_level\n",
    "lower_bound = bootstrap_means[int(alpha * num_samples)]\n",
    "upper_bound = bootstrap_means[int((1 - alpha) * num_samples - 1)]\n",
    "print(\"Original sample mean:\", sum(tree_heights) / len(tree_heights))\n",
    "print(\"95% confidence interval for population mean height:\", lower_bound, \"-\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb737440",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
