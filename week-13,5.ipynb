{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb0a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to transform numerical features into a common range. This method scales the values of a feature to a specified range, typically between 0 and 1, but it can be customized to any desired range. Min-Max scaling is useful when you want to ensure that all features have the same scale, preventing certain features from dominating others in machine learning algorithms that rely on distance or magnitude, such as gradient descent or K-means clustering.\n",
    "\n",
    "The formula for Min-Max scaling is as follows for a single feature:\n",
    "\n",
    "Xnew=(X-min(X))/max(X)-min(X) \n",
    "\n",
    "Where:\n",
    "Xnew is the scaled value of the feature.\n",
    "\n",
    "X is the original value of the feature.\n",
    "\n",
    "min(X) is the minimum value of the feature.\n",
    "\n",
    "max(X) is the maximum value of the feature.\n",
    "\n",
    "example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Suppose you have a dataset with a numerical feature, \"House Area,\" which represents the size of houses in square feet. The original values of this feature vary between 800 square feet and 3,000 square feet.\n",
    "\n",
    "Original House Area values:\n",
    "House 1: 1200 square feet\n",
    "House 2: 1800 square feet\n",
    "House 3: 2500 square feet\n",
    "To apply Min-Max scaling to these values and transform them into a range between 0 and 1:\n",
    "\n",
    "Find the minimum and maximum values of the \"House Area\" feature:\n",
    "\n",
    "min(X)=800 \n",
    "Mmax(X)=3000\n",
    "Apply the Min-Max scaling formula to each value:\n",
    "House 1(1200 square feet):\n",
    "    Xnew=(1200-800)/(300-800)=400/2200 =0.1818\n",
    "\n",
    "House 2 (1800 square feet):\n",
    "    Xnew=(1800-800)/(3000-800)=0.4545\n",
    "\n",
    "House 3 (2500 square feet):\n",
    "    Xnew=(2500-800)/(3000-800)=0.7727\n",
    "\n",
    "After Min-Max scaling, the \"House Area\" values are transformed to the range [0, 1]:\n",
    "\n",
    "House 1: 0.1818\n",
    "House 2: 0.4545\n",
    "House 3: 0.7727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e01d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648aa851",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Unit Vector technique, also known as vector normalization or unit length scaling, is a feature scaling method used to transform numerical features into unit vectors. Unlike Min-Max scaling, which scales features to a predefined range (typically [0, 1] or [-1, 1]), the Unit Vector technique scales features to have a length or magnitude of 1. This normalization method is often used in machine learning algorithms that rely on the direction or angle between data points rather than their absolute values.\n",
    "\n",
    "The formula for unit vector scaling for a single feature is as follows:\n",
    "\n",
    "Xnew=X/||X||\n",
    "\n",
    "Suppose you have a dataset with numerical features representing the coordinates of points in a 2D space. One of the features is \"X-coordinate,\" and another is \"Y-coordinate.\" You want to scale these features into unit vectors.\n",
    "\n",
    "Original data:\n",
    "\n",
    "Point 1: (3, 4)\n",
    "Point 2: (1, 2)\n",
    "To apply the Unit Vector technique to these data points:\n",
    "\n",
    "Calculate the Euclidean norm of each data point:\n",
    "\n",
    "For Point 1: ∥X∥=√(9+16)=5\n",
    "\n",
    "For Point 2: ∥X∥=For Point 2: √(1`+4)=√5 \n",
    "        \n",
    "Apply the Unit Vector scaling formula to each data point:\n",
    "\n",
    "Point 1:\n",
    "   Xnew=(3/5,4/5)\n",
    "    \n",
    "Point 2:\n",
    "  Xnew=(1/√5 ,2/√5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b992ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e124a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA, or Principal Component Analysis, is a dimensionality reduction technique used in data analysis and machine learning. Its primary goal is to reduce the number of features (or dimensions) in a dataset while preserving as much of the original information as possible. PCA achieves this by transforming the data into a new coordinate system, where the first few principal components capture the most significant variation in the data.\n",
    "\n",
    "Here's a step-by-step explanation of how PCA works and its application:\n",
    "\n",
    "1)Standardization:Start by standardizing or normalizing the features if they are on different scales. This step ensures that all features contribute equally to the PCA analysis.\n",
    "\n",
    "2)Covariance Matrix:Compute the covariance matrix of the standardized data. The covariance matrix describes the relationships and dependencies between pairs of features.\n",
    "\n",
    "3)Eigenvalue Decomposition:Calculate the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues represent the amount of variance explained by each corresponding eigenvector.\n",
    "\n",
    "4)Sort Eigenvalues:Sort the eigenvalues in descending order. The eigenvector associated with the highest eigenvalue explains the most variance in the data, the next highest explains the second most, and so on.\n",
    "\n",
    "5)Select Principal Components:Choose the top k eigenvectors (principal components) based on how much variance you want to retain in your reduced-dimensional data. Typically, you aim to retain a high percentage of the total variance, e.g., 95% or 99%.\n",
    "\n",
    "6)Project Data:Project the original data onto the selected principal components to create a new feature space. Each data point is represented by a linear combination of the principal components.\n",
    "\n",
    "7)Dimensionality Reduction:The reduced-dimensional dataset contains only the top k principal components, effectively reducing the dimensionality of the data.\n",
    "    \n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "np.random.seed(0)\n",
    "data = np.random.randn(100, 3)  \n",
    "pca = PCA(n_components=2)\n",
    "transformed_data = pca.fit_transform(data)\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Transformed Data (2D):\")\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2bfd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b552a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PCA (Principal Component Analysis) and feature extraction are closely related concepts in the field of dimensionality reduction and data preprocessing. PCA can be used as a technique for feature extraction, where it transforms the original features into a new set of features that capture the most important information in the data. This transformation can help reduce dimensionality while retaining meaningful information for modeling or analysis.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Original Features:Start with a dataset containing a set of original features, often with high dimensionality.\n",
    "\n",
    "Standardization:If necessary, standardize or normalize the original features to ensure they are on a common scale.\n",
    "\n",
    "PCA Transformation:Apply PCA to the standardized features to compute the principal components. PCA finds linear combinations of the original features that capture the maximum variance in the data.\n",
    "\n",
    "Feature Extraction:The principal components themselves serve as the new features. They are ranked by the amount of variance they explain in the data. The first few principal components capture the most variation, and they are used as the extracted features.\n",
    "\n",
    "Reduced Dimensionality:The number of principal components chosen determines the reduced dimensionality of the data. You can select a subset of the principal components to achieve the desired level of dimensionality reduction.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "data = np.array([[1, 2, 3, 4],\n",
    "                 [4, 3, 2, 1],\n",
    "                 [2, 3, 2, 4],\n",
    "                 [3, 2, 3, 1]])e\n",
    "transformed_data = pca.fit_transform(data)\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nTransformed Data (2D):\")\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbf4714",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f2aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "Min-Max scaling is a data preprocessing technique that can be applied to features with different scales to bring them into a common range, typically between 0 and 1. In the context of building a recommendation system for a food delivery service, you can use Min-Max scaling to preprocess features like price, rating, and delivery time to ensure they have similar scales. Here's how you would use Min-Max scaling for this project:\n",
    "\n",
    "Understand the Data:First, you should have a clear understanding of the dataset and the features you're working with. In your case, you have features like price, rating, and delivery time that may have different ranges and units.\n",
    "\n",
    "Import Libraries:Import the necessary libraries, such as NumPy or scikit-learn, to perform Min-Max scaling.\n",
    "\n",
    "Data Preprocessing:If your dataset has any missing values or outliers in the features you plan to scale, handle them appropriately. Impute missing values and apply outlier detection and treatment techniques if necessary.\n",
    "\n",
    "Repeat for Each Feature:Apply the Min-Max scaling process separately to each feature you want to scale (e.g., price, rating, and delivery time).\n",
    "\n",
    "Create a Preprocessed Dataset:Once you have scaled all the relevant features, you can create a new dataset or update the existing one with the scaled feature values.\n",
    "\n",
    "Normalization Parameters:It's important to keep track of the parameters used for scaling (i.e., the minimum and maximum values) because you'll need them when making predictions or recommendations. Store these parameters so that you can revert the scaling if necessary.\n",
    "\n",
    "Use the Preprocessed Data:The preprocessed data with scaled features can now be used as input for building your recommendation system. You can apply various recommendation algorithms, such as collaborative filtering, content-based filtering, or hybrid approaches, depending on your project's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974d585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ea434",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Using PCA (Principal Component Analysis) for dimensionality reduction in a project to predict stock prices can be a valuable preprocessing step. Reducing the dimensionality of the dataset can help improve model performance, reduce computational complexity, and remove multicollinearity among features.\n",
    "how you would use PCA for this purpose:\n",
    "\n",
    "Data Preprocessing:Start by cleaning and preprocessing your dataset. This may include handling missing values, encoding categorical variables, and standardizing or normalizing numerical features.\n",
    "\n",
    "Select Relevant Features:Before applying PCA, it's crucial to identify the features that are most relevant for predicting stock prices. You can use domain knowledge, statistical analysis, or feature selection techniques to narrow down the feature set. Removing irrelevant features can improve PCA's effectiveness.\n",
    "\n",
    "Standardization or Normalization:Ensure that your selected features are on the same scale. Standardization (mean centering and scaling to unit variance) or Min-Max scaling can be used to achieve this. PCA is sensitive to the scale of features, so standardization is often recommended.\n",
    "    \n",
    "Dimensionality Reduction:The reduced-dimensional dataset obtained from PCA contains the transformed features, which are linear combinations of the original features. These components are orthogonal and capture the most important patterns in the data while reducing dimensionality.\n",
    "\n",
    "Feature Interpretation:Examine the principal components to understand how they relate to the original features. You can interpret the weights (loadings) of the original features on each principal component to gain insights into the underlying patterns. This interpretation can help you understand which financial and market factors contribute most to the variation in stock prices.\n",
    "\n",
    "Model Building:Use the reduced-dimensional dataset as input for building your stock price prediction model. You can apply various regression techniques, time series models, or machine learning algorithms to develop your predictive model. The reduced feature space can lead to faster training times and potentially better model performance.\n",
    "\n",
    "Model Evaluation and Tuning:Evaluate the performance of your stock price prediction model using appropriate evaluation metrics. You may need to fine-tune your model and experiment with different hyperparameters to achieve the best results.\n",
    "\n",
    "Monitoring and Updating:Continuously monitor the performance of your model and the relevance of the selected principal components. Stock market dynamics can change over time, and you may need to update your model and feature selection strategy accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53826fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Min-Max Scaled Data: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "min_value = np.min(data)\n",
    "max_value = np.max(data)\n",
    "scaled_data = (data - min_value) / (max_value - min_value)\n",
    "scaled_data = (scaled_data * 2) - 1\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Min-Max Scaled Data:\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2950d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2cd66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The decision of how many principal components to retain when performing PCA (Principal Component Analysis) for feature extraction depends on your specific goals and the explained variance you want to retain. Here's a general process to help you decide:\n",
    "\n",
    "Standardization:Start by standardizing the features (height, weight, age, gender, blood pressure) if they are on different scales. PCA is sensitive to the scale of features, so standardization is often necessary.\n",
    "\n",
    "Covariance Matrix:Calculate the covariance matrix of the standardized features. This matrix describes the relationships between pairs of features.\n",
    "\n",
    "Eigenvalue Decomposition:Compute the eigenvalues and eigenvectors of the covariance matrix. These eigenvalues represent the amount of variance explained by each principal component, and the eigenvectors represent the direction of each component.\n",
    "\n",
    "Sort Eigenvalues:Sort the eigenvalues in descending order. The principal components associated with higher eigenvalues capture more variance in the data.\n",
    "\n",
    "Explained Variance:Calculate the cumulative explained variance, which tells you how much of the total variance is explained by the first k principal components. This helps you decide how many components to retain.\n",
    "\n",
    "Select the Number of Components:Decide on the number of principal components to retain based on your project's goals. Common choices include:Retaining enough components to explain a certain percentage of the total variance (e.g., 95% or 99%).\n",
    "=>Choosing the number of components that capture the elbow point in the explained variance plot.\n",
    "\n",
    "Transform the Data:Finally, project the original data onto the selected principal components to create a reduced-dimensional dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
